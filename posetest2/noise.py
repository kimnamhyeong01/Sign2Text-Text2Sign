import os
import cv2
import numpy as np
import torch
import torch.nn.functional as F
from model import PoseFormer  # ë„¤ ëª¨ë¸ í´ë˜ìŠ¤

# âœ… ëª¨ë¸ ì •ì˜ ë° ë¡œë“œ
model = PoseFormer()
model.load_state_dict(torch.load("E:/pose/pose_transformer_finetuned.pt", map_location='cpu'))
model.eval()

# âœ… Mediapipe ì„¤ì •
import mediapipe as mp
mp_pose = mp.solutions.pose
mp_hands = mp.solutions.hands
pose = mp_pose.Pose(static_image_mode=False)
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2)

TARGET_FRAMES = 30
TOTAL_KEYPOINTS = 66

# âœ… Keypoint ì¶”ì¶œ í•¨ìˆ˜
def extract_pose_from_video(video_path):
    def extract_landmarks(results_pose, results_hands):
        keypoints = []
        if results_pose.pose_landmarks:
            keypoints.extend([[l.x, l.y] for i, l in enumerate(results_pose.pose_landmarks.landmark) if i <= 23])
        else:
            keypoints.extend([[0.0, 0.0]] * 24)

        lh, rh = [[0.0, 0.0]] * 21, [[0.0, 0.0]] * 21
        if results_hands.multi_hand_landmarks:
            for hand_landmarks, handedness in zip(results_hands.multi_hand_landmarks, results_hands.multi_handedness):
                label = handedness.classification[0].label
                if label == 'Left':
                    lh = [[l.x, l.y] for l in hand_landmarks.landmark]
                elif label == 'Right':
                    rh = [[l.x, l.y] for l in hand_landmarks.landmark]
        keypoints.extend(lh + rh)
        return keypoints

    def sample_or_pad_sequence(seq, target_len):
        n = len(seq)
        if n == 0:
            return np.zeros((target_len, TOTAL_KEYPOINTS, 2))
        if n == target_len:
            return np.array(seq)
        elif n > target_len:
            idxs = np.linspace(0, n - 1, target_len).astype(int)
            return np.array([seq[i] for i in idxs])
        else:
            pad_len = target_len - n
            return np.array(seq + [seq[-1]] * pad_len)

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"âŒ ì˜ìƒ ì—´ê¸° ì‹¤íŒ¨: {video_path}")

    frame_keypoints = []
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results_pose = pose.process(frame_rgb)
        results_hands = hands.process(frame_rgb)
        keypoints = extract_landmarks(results_pose, results_hands)
        frame_keypoints.append(keypoints)

    cap.release()

    if not frame_keypoints:
        raise ValueError("âŒ ìœ íš¨í•œ keypointë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    return sample_or_pad_sequence(frame_keypoints, TARGET_FRAMES)

# âœ… ì˜ˆì¸¡ í•¨ìˆ˜
def predict_from_keypoints(kp_array):
    keypoints = torch.tensor(kp_array, dtype=torch.float32).view(1, 30, -1)
    with torch.no_grad():
        output = model(keypoints)
        prob = F.softmax(output, dim=1)
        return torch.argmax(prob, dim=1).item()

# âœ… ê²½ë¡œ ì„¤ì •
video_path = "E:/video/KETI_SL_0000000217.avi"
npy_path = "E:/pose/217.npy"

# âœ… ì¶”ì¶œ â†’ ì˜ˆì¸¡
pose_from_video = extract_pose_from_video(video_path)
pred_from_video = predict_from_keypoints(pose_from_video)

# âœ… ê¸°ì¡´ npy â†’ ì˜ˆì¸¡
pose_from_npy = np.load(npy_path)
pred_from_npy = predict_from_keypoints(pose_from_npy)

# âœ… ê²°ê³¼ ë¹„êµ
print(f"ğŸ¥ ì˜ìƒ ì¶”ì¶œ ì˜ˆì¸¡ ê²°ê³¼   â†’ {pred_from_video}")
print(f"ğŸ“ ê¸°ì¡´ NPY ì˜ˆì¸¡ ê²°ê³¼   â†’ {pred_from_npy}")
print("âœ… ì¼ì¹˜ ì—¬ë¶€:", "âœ”ï¸ ë™ì¼" if pred_from_video == pred_from_npy else "âŒ ë‹¤ë¦„")
